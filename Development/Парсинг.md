>[!warning] API ключи, токены и тп храним в отдельном файле.

Сайты для проверки:

|Сайт|Описание|
|-------|----------|
|https://jsonplaceholder.typicode.com/|можно потренировать обработку json
|https://httpbin.org/headers/|можно проверить свои запросы
|https://curlconverter.com/python/|конвертирует curl-запрос в код Python(Самый главный GET запрос копируем в панели разраб. как cURL)

```
pip install requests beautifulsoup4 lxml
```

```
from bs4 import BeautifulSoup

params = {"q", "funny cats"}                  # словарь с параметрами
headers = {
    "Accept": "*/*",
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari 537.36"
    }                                         # здесь можно подменить заголовки(напр. User-Agent)

# объект с ответом
response = requests.get("https://www.google.ru/", params=params, headers=headers)

response.headers()                                     # заголовки
        .content()                                     # содержимое в виде байтовой строки
        .text()                                        # HTML код страницы
        .json()                                        # возвращает словарь json
        .status_code()                                 # код ответа сервера(200, 404 и т.д.)

# уже обработанная html страница, чтобы удобнее было проводить навигацию
soup = BeautifulSoup(response.text, 'lxml')

# получаем объект Soup
data = soup.find_all('div', class_='card mt-4 my-4')
```

#### Нахождение объектов
```
soup.find('div' class_='aaa')               # найти первый такой тэг
soup.find_all('div' class_='aaa')           # найти все такие тэги (recurive=False для просмотра 
                                            #  только верхнего уровня)
.find_all(['h1', 'h2', 'h3'])               # найти все теги из списка     
.find_all(lambda tag: tag.get_text() == 'Награды') # найти теги, содержащие определённый текст(лямбда функция должна принимать тег, а возвращать булево значение)
.find('div' class_='aaa').parent()          # найти такой тэг и его родителя
.find('div' class_='aaa').parents()         # найдёт тэг и всех родителей(включая <html>)
.next_element()                             # след. элемент(это может быть перенос строки)
.find_next()                                # след. элемент(без переноса строки)
.previous_element()                         # пред. элемент
.find_next_sibling()                        # след. одноуровневый элемент
.find_previous_sibling()                    # пред. одноуровневый элемент
.h1.get_text()                              # получить тег h1 и получить текст из тега
.select("body a")                           # доступ к элементам по селекторасм CSS (все ссылки внутри body)
.select(".sister")                          # теги по классу CSS sister
.find('href' class_='aaa').get('src')       # получить атрибут тэга
.get_text()                                 # Удаляет все теги и возвращет текст в Unicode
.find('select', id='USER_STREET_ID').select_one('option:checked') # Найти выбранный пункт из выпадающего спи
```

#### XPath
![[Pasted image 20241110010134.png]]
>[!info] Для навигации и поиска внутри XML используется **язык запросов XPath**.

>[!tip] Быстро проверить выражение в браузере можно набрав в консоли
```
$x("//div[@class='quote']")
```

Или скопировать из браузера
![[d4225331e73538ba9b32a883ad0317de.webp]]

[XPath cheat sheet](https://devhints.io/xpath)
[Примеры запросов](https://habr.com/ru/articles/753332/)

###### Абсолютные пути
|Запрос|Результат|
|---|---|
|`/courses/course`|Все данные из всех элементов `<course></course>`|
|`/courses/course/name`|`<name>Основы современной верстки</name>`  <br>`<name>Основы верстки контента</name>`  <br>`<name>Bootstrap 5: Основы верстки</name>`|
|`/courses/course/duration`|`<duration value="9">9 часов</duration>`  <br>`<duration value="18">18 часов</duration>`  <br>`<duration value="10">10 часов</duration>`|

###### Относительные пути
>[!info] Поиск по всему XML и возврат узлов, подходящих под запрос.

|Запрос|Результат|
|---|---|
|`//course`|Все данные из всех элементов `<course></course>`|
|`//name`|`<name>Основы современной верстки</name>`  <br>`<name>Основы верстки контента</name>`  <br>`<name>Bootstrap 5: Основы верстки</name>`|
|`//course/duration`|`<duration value="9">9 часов</duration>`  <br>`<duration value="18">18 часов</duration>`  <br>`<duration value="10">10 часов</duration>`|

###### Предикаты
Взять первый элемент
```
//course[1]/tags
```
получить элементы `<url>`, у которых атрибут `lang` равен `ru`
```
//course/url[@lang="ru"]
```
элементы `<duration>` со значением атрибута `value` больше `9`
```
//course/duration[@value > 9]
```

>[!info] 
>- Предикат необязательно должен идти в конце запроса
>- Внутри предиката могут находиться новые пути, которые нужно проверить

###### Функции
>[!info] Являются частью предикатов — например, `@`

запрос для поиска курса с нужным именем
```
//course[name[text()="Основы верстки контента"]]
```
курс, у которого в ключевых словах есть слово «Bootstrap»
```
//course[tags[contains(text(), "Bootstrap")]]
```

#### Итерация
###### Простейшая итерация по элементам
```
for i in soup.find_all('img'):
        print(i['src'])
```

###### Простейшая итерация по детям
```
for i in soup.find('ul').children:
        print(i)
```

###### Простейшая итерация по одноуровневым элементам(исключая запрашиваемый)
```
for i in soup.find('li').next_siblings:
        print(i.get_text())
```

>[!info] По хорошему нужно проверять соединение с доменом. Проверять существует ли элемент. А потом ещё проверять наличие тега у элемента

>[!tip] Если на сайте большая вложенность и чтобы достать нужный элемент нужно писать моструозные конструкции, типа `bs.find_all('table')[4].find_all('tr')[2].find('td').find_all('div')[1].find('a')`, то:
1) Можно поробывать найти мобильную версию с более удачным html-форматированием
2) Поискать информацию в файле JavaScript

#### POST-запросы
```
data = {'login': 'asdf', 'password': '12345'}

response = requests.post("https://www.google.ru/", data=data)
```
###### Логин с помощью POST-запроса
```
from requests import Session
from bs4 import BeautifulSoup
from time import sleep

headers = {"User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari 537.36"}   

# requests позволяет работать с куками
work = Session()

# имитируем заход на главную с подменой заголовков
work.get("https://quotes.toscrape.com/", headers=headers)

# заходим на страницу авторизации и получаем её
response = work.get("https://quotes.toscrape.com/login", headers=headers)

# разбираем вспомогательным(lxml) парсером страницу авторизации
soup = BeautifulSoup(response.text, 'lxml')

# получаем значение скрытого одноразового токена со страницы
token = soup.find('form').find('input').get('value')

# задаём словарь, который будет уходить с POST-запросом на авторизацию
data = {'csrf_token': token, 'username': 'noname', 'password': 'password'}

# отправляем POST-запрос с нашими данными(и верным токеном), позволяем редиректы и получаем залогиненную страницу
result = work.post("https://quotes.toscrape.com/login", headers=headers, data=data, allow_redirects=True)
```

#### Получение cookie
```
r = requests.get('https://www.google.ru/')
print(r.cookies.get_dict())

r = requests.get('https://www.google.ru/', cookies=r.cookies)
```

#### Сессии
```
variable = requests.Session()                                # В сессии сохраняются куки и др.
aaa = variable.get("https://www.google.ru/")
response = requests.get("https://www.google.ru/", params=params, headers=headers, allow_redirects=True)
```

#### Пример поиска товаров на одной странице
```
# найдём все карточки товаров на одной странице. 
# Если ищем на многих страницах, ставим sleep между запросами
pip install requests, beautyfulsoup4, lxml                  

# уже обработанная html страница, чтобы удобнее было проводить навигацию
soup = BeautifulSoup(response.text, 'lxml')

data = soup.find_all('div', class_='card mt-4 my-4')

for i in data:
    name = i.find('h3', class_='card-title').text.replace('\n', '')
    price = i.find('h4').text
    url_img = "https://scrapingclub.com" + i.find('img', class_="card-img-top img-fluid").get('src')
    print(name, price, url_img, '\n\n')
```

#### Пример поиска товаров на восьми страницах
```
# поиск всех карточек товаров на 8ми страницах

import requests
from bs4 import BeautifulSoup
from time import sleep

headers = {"User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari 537.36"}   

def get_url():
    for count in range(1, 8):
        url = f'https://scrapingclub.com/exercise/list_basic/?page={count}'
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'lxml')
        data = soup.find_all('div', class_='col-lg-4 col-md-6 mb-4')

        for i in data:
            card_url = "https://scrapingclub.com" + i.find('a').get('href')
            yield card_url


for card_url in get_url():
    response = requests.get(card_url, headers=headers)
    sleep(3)
    soup = BeatifulSoup(response.text, 'lxml')

    data = soup.find('div', class_='cardd mt-4 my-4')
    name = data.find('h3', class_='card_title').text
    price = data.find('h4').text
    text = data.find('p', class_='card-text').text
    url_img = "https://scrapingclub.com" + data.find('img', class_='card-img-top img-fluid').get('src')
```

#### Загрузка файла
```
# загрузка файла

def download(url):
    resp = requests.get(url, stream=True)           # потоковая(порционная) загрузка
    r = open('/home/ufodriver/img', 'wb')           # запись побайтово
    for value in resp.iter_content(1024 * 1024):    # порицией в 1 Мб
        r.write(value)
    r.close()
```

#### Добавление прокси
Создаём отдельный файл, например proxy_auth.py. Там создаём словарь:
```
proxies = {"https": "your_proxy_ip:port"}
```

В основном файле:
```
from proxy_auth import proxies

url = "blabla"
headers = "blabla"
req = requests.get(url=url, headers=headers, proxies=proxies)
```

#### Парсинг динамических страниц сайтов с JavaScript Ajax
- Динамически подгружаемый контент можно видеть в инспекторе->Network->Fetch/XHR в Payload, Response и т.п.
- Смотрим какой запрос посылается при прокрутке.
- Всегда смотрим в документ базовой страницы Network->All ищем там его по типу document или html
- Проверяем в его заголовке, что код 200(перенаправлений нет).
- Можно попробывать обнаружить скрытую навигацию в базовой странице(или она мб уже открыта) и тогда в цикле
- Читаем и парсим контент. Навигация может быть даже в куках!

#### User-Agent
Библиотека для упрощения работы с User-Agent
```
pip install fake-useragent
```

```
from fake-useragent import UserAgent
useragent = UserAgent()
print(useragent.ie)   
```

#### Selenium
Библиотека для управления браузерами.

```
pip install selenium
```

```
import time
from selenium import webdriver
from selenium.webdriver.firefox.service import Service

# пример получения страницы с использованием драйвера Chrome
options = webdriver.ChromeOptions()  
options.add_argument('--headless')  

driver = webdriver.Chrome(options=options)
driver.get("https://eu.account.battle.net/creation/flow/creation-full")
email_input = driver.find_element(by=By.ID, value='capture-email')  
email_input.send_keys('asdf@ya.ru')

submit = driver.find_element(by=By.ID, value='flow-form-submit-btn')  
submit.click()

driver.quit()
```

```
driver.get(url=url)                    # Перейти на страницу
driver.refresh()                       # Обновить страницу
driver.current_url                     # Текущий url
driver.implicitly_wait(5)              # Установка общего таймаута(ожидать элемент не более 5 сек)
```

###### Поиск элемента на странице
```
from selenium.webdriver.common.by import By

email_input = driver.find_element(By.NAME, 'q')
```

###### Управление куками в selenium
```
import pickle

# save_cookie
pickle.dump(driver.get_cookies(), open("007_cookies", "wb"))

# load_cookie
for cookie in pickle.load(open("007_cookies", "rb")):
    driver.add_cookie(cookie)
```

###### Отключение режима webdriver
```
#disable webriver mode
options.set_preference("dom.webdriver.enabled", False)
```

###### Нажатие кнопок
```
search_button.click()
```

нажатие спец. кнопок
```
from selenium.webdriver.common.keys import Keys

email_input.send_keys(Keys.ENTER)
```

###### headless mode
```
options.add_argument('-headless')
        или
options.headless = True                 # deprecated
```

###### Вкладки
```
# список со вкладками
print(driver.window_handles)

# переход по вкладкам
driver.switch_to.window(driver.window_handles[0])

# закрытие вкладки, сразу переходим на другую вкладку, иначе будет ошибка
driver.close()
driver.switch_to.window(driver.window_handles[0])

# создание вкладки
driver.get(url="https://www.google.com")
```

###### Создание нескольких процессов Selenium
```
from multiprocessing import Pool

def get_data(url):
    # код парсера страницы
    pass

url_list = ["https://google.ru", "https://youtube.com", "https://selenium-python.readthedocs.io"]
    
p = Pool(processes=3)
p.map(get_data, url_list)
```

#### CLODFLARE
>[!info] Можно попытаться обойти с помощью undetected_chromedriver(обёртка для Selenium)

```
pip install undetected_chromedriver
```

```
import undetected_chromedriver
driver = undetected_chromedriver.Chrome()
driver.get(url)
```

>[!tip] Можно скопировать главный GET-запрос как  cURL и в [конвертере](https://curlconverter.com/python/) получить Python код с уже готовыми куками и заголовками

#### Пример асинхронного кода
```
async def fetch_content(url, session):
    """GET-зпарос и вызов синхронной функции записи в файл"""
    async with session.get(url, allow_redirects=True) as response:
        data = await response.read()
        write_image(data)


async def main():
    """Создание очереди задач"""
    url = 'http://loremflickr.com/320/240'
    tasks = []

    async with aiohttp.ClientSession() as session:
        for i in range(10):
            task = asyncio.create_task(fetch_content(url, session))
            tasks.append(task)

        await asyncio.gather(*tasks)


def write_image(data):
    """Запись в файл"""
    filename = f'file-{int(time() * 1000)}.jpeg'
    with open(filename, 'wb') as file:
        file.write(data)


# Запуск событийного цикла
asyncio.run(main()) 
```

###### Ещё пример
Пример 100 асинхронных запросов на сервер(нужно использовать `aiohttp` потому что `requests` синхронная библиотека)

```
import aiohttp  
import asyncio  
from bs4 import BeautifulSoup  
import cProfile  
  
  
async def request(url, session):  
    async with session.get(url) as response:  
        html = await response.text()  
        soup = BeautifulSoup(html, 'lxml')  
        title = soup.find('title').text  
        return title  
  
  
async def request_manager():  
    url = 'https://www.mint-coast.ru/'  
    tasks = []  
  
    async with aiohttp.ClientSession() as session:  
        for _ in range(100):  
            task = asyncio.create_task(request(url, session))  
            tasks.append(task)  
  
        results = await asyncio.gather(*tasks)  
  
    return results  
  
  
cProfile.run('asyncio.run(request_manager())')  # 1.6 seconds average
```

###### Паттерн для асинхронного парсера
```
async def get_pages_refs():
    """Собираем ссылки на нужные страницы с товарами""""
    tasks = []
    refs = []
  
    # GET-запрс
    async with aiohttp.ClientSession() as session:
        response = await session.get(url=url, headers=headers)
        soup = BeautifulSoup(response_text, 'lxml') 
    
        # получение ссылок с товарами
        links = soup.findAll('a', class_="prod_a")
        for link in links:
            refs.append([f"https://mircli.ru{link.get('href')}"])
        
        # Создание очереди задач
        for ref in refs:
            task = asyncio.create_task(get_page_data(session, ref, category))
            tasks.append(task)

        await asyncio.gather(*tasks)


async def get_page_data(session, ref, category):
    """Собираем информацию о конкретном товаре""""
    async with session.get(url=ref, headers=headers) as response:
        response_text = await response.text()
        soup = BeautifulSoup(response_text, 'lxml')

        name = soup.find('h1', class_="prod_a")
        price = soup.find('div', class_="price")
        description = soup.find('div', class_="description")

        with open('result.csv', 'a') as f:
            csvout = csv.writer(f)
            csvout.writerow([name, price, description])


# Запуск событийного цикла
asyncio.run(get_pages_refs())
```

## Scrapy
[[Scrapy]]